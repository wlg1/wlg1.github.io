I"ı$<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<ol>
  <li>Cov matrix eigenvectors meaning? There is no permanent meaning; the matrix is just a tool of where to send concepts to (to re-frame persepctive).</li>
</ol>

<p>after multp cov matrix by orig, eigenvec (lin combo of orig) just stretches.</p>

<p><strong>Stretching the eigenvdctors is the same as applying the matrix</strong></p>

<p>Diagonal can only be done on basis. So Q transpose is to get eigenvdctors onto basis.</p>

<p>Only square symmetric have orthogonal eigenvdctors. So to get sq sym for A, do AAtr and AtrA.</p>

<p>Â«&lt;
https://www.youtube.com/watch?v=mhy-ZKSARxI&amp;ab_channel=VisualKernel</p>

<p>roate to get eigenvec to stnd basis so can stretch by diag. when eigenvectors on diag, diag of eigenvals stretches the eigenvectors. thatâ€™s the reason thereâ€™s a diag.</p>

<p>eigenvec are â€˜diagsâ€™ of the square (orig)</p>

<p>Â«&lt;
3a. spine length eigenvector
3b. eigenvectors of cov matrix are vectors of lin combo of how each feature affects that colâ€™s feature
3c. scaling affects whatâ€™s currently on basis
    why doesnâ€™t QQ^-1 = I? try lambda<em>Q; itâ€™s the same as Q</em>lambda</p>

<p>cov pts are pairs, and each axis is one of the orig x data pts (not the features of each x). pt (x1,x2): the proj along one of the orig data pts x1 shows how far away x2 is from x1. thus, an eigenvector of cov matrix is a lin combo of orig data pts (in which every x2â€¦?)</p>

<p>or is it lin combo of orig features of x (not data pts)? cov matrix is multiplied by orig space, but are eigenvectors of the matrixâ€™s basis or of the basis the matrix is mulpt by?</p>

<p>Â«&lt;
Is the eigenvector in the column of Q?
https://www.d.umn.edu/~mhampton/m4326svd_example.pdf
Yes, but itâ€™s the eigenvectors of X*X = A = QLQ^-1, not of X</p>

<p>https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix
where Q is the square n Ã— n matrix whose ith column is the eigenvector qi of A</p>

<p>(this is the anchor pt of the analogical reasoning)
(see fig1 in ch1.2)
Since the columns of matrix Q are the NEW coordinates of the concepts that were on the basis vectors, then multiplying by Q means the concepts that were on the basis vectors are now sent to the eigenvectors.</p>

<p>In terms of body + face â€“&gt; In terms of cat + rat
In terms of body + face â€“&gt; In terms of eigenvectors</p>

<p>Q matrix:
face_ev1, body_ev1
face_ev2, body_ev2</p>

<p>So if the eigenvector is â€˜catâ€™, we interpret the data in terms of cat. All we need to do to interpret in terms of cat is to use the eigenvalues (?)</p>

<p>We use cov=Q<em>lambda</em>Q^-1. Q^-1 sends concepts on the eigenvectors to the basis vecs. 
Since the columns of matrix Q are the NEW coordinates of the concepts that were on the basis vectors, the columns of Q^-1 sends the concepts on the basis vectors somewhere, as long as it allows the concepts from the eigenvectors to get to the basis vectors.</p>

<p>After Q^-1 sends the concepts on the basis vectors â€˜somewhereâ€™, lambda stretches them, then Q sends these concepts back to basis vecs, meaning that the concepts originally from the eigenvectors are no longer on the basis vectors, but are back on the eigenvectors.</p>

<p>Eg) Originally, face and body were on the basis vectors, and â€˜catâ€™ was on the eigenvectors. First, Q^-1 sends â€˜catâ€™ and â€˜ratâ€™ to the basis vectors, and sends face and body â€˜somewhereâ€™. Then, Lambda stretches â€˜catâ€™ and â€˜ratâ€™, moving two concepts â€˜pâ€™ and â€˜qâ€™ onto the basis vectors (cat and rat are NO LONGER ON THE BASIS VECTORS). Finally, Q sends concepts â€˜pâ€™ and â€˜qâ€™ onto eigenvectors ev1 and ev2.</p>

<p>Whatâ€™s important is that to remember that vectors and concepts are different- thus, what was on the eigenvectors is DIFFERENT from the eigenvectors themselves. Before cov = Q<em>L</em>Q^-1, the eigenvectors ev1 and ev2 had â€˜catâ€™ and â€˜ratâ€™ on them. Afterwards, they have new concepts â€˜pâ€™ and â€˜qâ€™, and cat and rat are no longer on the eigenvectors. It is not right to assume that Q sends cat and rat back to the eigenvectors. The eigenvectors, contrary to an easily mistaken assumption, do not always have the same concepts on it!</p>

<p>QUESTION- if the concepts on an eigenvector are merely stretched, how does Q rotate p from basis onto eigenvector?
ANSWER- itâ€™s not about AFTER THE Q TRANSFORM, ITâ€™S ABOUT AFTER THE A (COV) TRANSFORM! This is another easily mistaken assumption, because â€˜after the transformâ€™ is ambiguous, so you think itâ€™s Q and/or A, when itâ€™s just A. As you see in this link:
https://math.stackexchange.com/questions/2816259/geometric-interpretation-of-eigendecomposition
After A, the concepts on the eigenvectors are stretched. They donâ€™t necessarily stay in the same place- theyâ€™re stretched!!! This is why theyâ€™re useful to orient back again- you know where theyâ€™re supposed to go originally, albeit stretched.</p>

<p>Why not just stretch eigenvecs? The reason is because we need to stretch whatâ€™s on the basis using lambda, so we need to get cat and rat onto the basis.</p>

<p>Ultimately, cov interprets the data in terms of cov. Remember, cov is how much one feature changes when another feature is changed.</p>

<p>The next step is to not take this example too strictly- when it comes to the cov matrix, whatâ€™s on the eigenvector wonâ€™t be â€˜catâ€™ or â€˜spine lengthâ€™.</p>

<p>Another tricky thing is that when you multp by cov, the PCs (eigenvectors) should be on basis- but we showed that theyâ€™re not. 
ANSWER- we donâ€™t multiply the data by cov, but by the eigenvectors of cov. This means we donâ€™t end up with cov on the basis vectors, but with the eigenvectors of cov on the basis. So we multiply the data by Q, not by A. The reason we do eigendecomposition on A is to find Q:</p>

<p>https://builtin.com/data-science/step-step-explanation-principal-component-analysis
â€œAn important thing to realize here is that the principal components are less interpretable and donâ€™t have any real meaning since they are constructed as linear combinations of the initial variables.â€</p>

<p>oddly enough, GANspace uses principal components to find interpretable edit directions</p>

<p>ev: â€œthe line in which the projection of the points (red dots) is the most spread out.â€</p>

<p>Now why do eigenvectors with the highest eigenvalues have â€˜the most informationâ€™ (most variance) about cov? Thatâ€™s its â€˜meaningâ€™.</p>

<p>https://www.youtube.com/watch?v=FD4DeN81ODY&amp;ab_channel=VisuallyExplained
3m: information preserved after projection onto the eigenvector is given by the eigenvalue</p>

<p>C = xx = cov
eigenvalue = uCu = EV<em>u</em>u = EV * I</p>

<p>https://math.stackexchange.com/questions/3211467/why-eigenvectors-with-the-highest-eigenvalues-maximize-the-variance-in-pca</p>

<p>Not all eigenvectors contain a lot of variance; only the ones with the highest eigenvalues</p>

<p>https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/
As we saw in figure 3, the covariance matrix defines both the spread (variance), and the orientation (covariance) of our data. So, if we would like to represent the covariance matrix with a vector and its magnitude, we should simply try to find the vector that points into the direction of the largest spread of the data, and whose magnitude equals the spread (variance) in this direction.</p>

<p>Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«
What are eigenvectors of neurons? A measurement thatâ€™s preserved. But remember, eigenvectors are just the vectors, NOT the concepts.</p>

<p>Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«
https://www.projectrhea.org/rhea/index.php/PCA_Theory_Examples
Does PCA multiply by PCs? Yes:
https://builtin.com/data-science/step-step-explanation-principal-component-analysis
the feature vector is a matrix that has as columns the eigenvectors of the components that we decide to keep. Reorient the data from the original axes to the ones represented by the principal components- This can be done by multiplying the transpose of the original data set by the transpose of the feature vector.</p>

<p>Â«Â«Â«Â«Â«Â«
https://math.stackexchange.com/questions/2830554/multiplying-a-matrix-with-its-eigenvectors-stretches-or-contracts-the-vector-wit</p>

<p>Â«Â«Â«Â«Â«&lt;
Multiplying by covariance matrix meansâ€¦
https://stats.stackexchange.com/questions/326508/intuitive-meaning-of-vector-multiplication-with-covariance-matrix
sum over the weighted covariances
 which means a value of how well Xi â€œcovariesâ€ in the direction of r.</p>

<p>https://www.researchgate.net/post/What-is-the-intuition-of-eigenvector-and-eigenvalue-from-a-control-system-perspective</p>

<p>Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«Â«
https://math.stackexchange.com/questions/2816259/geometric-interpretation-of-eigendecomposition
the orthogonal matrix U determines in which directions this scaling happens</p>

<p>this shows eigenvec of cov:
https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/
The eigenvalues still represent the variance magnitude in the direction of the largest spread of the data (just for cov matrix, not any matrix?)</p>

<p>Â«&lt;
https://www.youtube.com/watch?v=yDpz0PqULXQ&amp;ab_channel=SteveBrunton
Robust Principal Component Analysis (RPCA)</p>
:ET