I"E<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><strong>CHAPTER 1.1</strong></p>

<p><a href="index.md">back</a></p>

<p><strong>(Reading time: 5 minutes)</strong></p>

<p>Let’s start with an example that will show us how matrix multiplication transforms data to reveal new insights. Say there’s a population of cats and rats, and we represent them in a dataset. However, the dataset is only able to measure two features: body size, and face length (or more specifically, snout length).</p>

<p><img src="/cob/fig1.PNG" alt="Figure 1: Cat" />
<!--- middle overlays shapes on top of actual cat. label bottom right as 'data sample'---></p>

<p>So every entity (a cat or rat) in the population is represented in the dataset as an data sample, which is an abstraction defined only by body size and face length, such that their values are measured in “units”. We can represent the data samples in this dataset as points in a coordinate space, using the two features as axes. The figure below shows the dataset on the left, and its coordinate space representation on the right. The top part shows the data points using numbers, and the bottom part shows how each data point corresponds to a data sample:</p>

<p><img src="/cob/fig2.PNG" alt="Figure 2" />
<img src="/cob/fig3.PNG" alt="Figure 3" />
<!---Make dataset image: first row is face, second row is body, third row is data pt (combo of both) using #s, then show in coord sys on right. Then in 2nd image, turn all numbers into imgs, and again show in coordsys on right.---></p>

<p>Let’s look at our coordinate space with only the images of unit 1:</p>

<!--- ![Figure 4](/cob/fig4.PNG) --->
<p><img src="/cob/fig4.png" width="800" height="500" /></p>

<p>Every data point is a combination of the “unit 1” points, which represent <img src="/cob/face1.PNG" width="50" height="40" /> and <img src="/cob/body1.PNG" width="50" height="40" />. For instance, the data point (0.5, 2), which represents <img src="/cob/cat.PNG" width="50" height="40" />, is a weighted combination of 0.5 * <img src="/cob/face1.PNG" width="50" height="40" /> and 2* <img src="/cob/body1.PNG" width="50" height="40" /></p>

<p><img src="/cob/fig5.PNG" alt="Figure 5: Linear Combination" /></p>

<p>If we see each data point as a vector, then every vector such as <img src="/cob/cat.PNG" width="50" height="40" /> is an addition of <img src="/cob/face1.PNG" width="50" height="40" /> and <img src="/cob/body1.PNG" width="50" height="40" />, which are <strong>basis vectors</strong>. Thus (0.5, 2) can also be represented as \(\begin{bmatrix} 0.5 \\ 2 \end{bmatrix} = 0.5 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + 2 \begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</p>

<p><img src="/cob/fig6.PNG" alt="Figure 6: Cat in Coordinate Space" />
<!---
[under each body size pic, number it so reader knows if it's 1, 2, etc. The 2 glues two 1s together, showing the border, the 0.5 shows the other half grayed out, etc. This is to indicate they're scaling the unit vector. But [cat pic] does not do this, since it's not always measured using bodysize or face length, it's just pure data that can be represented using different features. you can show the addition with an 'intermediate step' that shows gluing them on, then removing the borders/faded. can also be gif]---></p>

<hr />

<p>Now, there are other ways we can measure the data samples in this population. Instead of labeling each data sample using the face &amp; body measurements, let’s label each data sample using the following two measurements:</p>

<p>1) How likely it is to be a cat</p>

<p>2) How likely it is to be a rat</p>

<p>How do we find the values of these new, and currently unknown, measurements? We can calculate them our previous known measurements, face &amp; body. For example:</p>

<p>Shorter face + Bigger body = Cat</p>

<p>Longer face + Smaller body = Rat</p>

<p>Or in terms of basis vector addition:</p>

<p>Fig 7
[figure showing the analogy: 
Bigger body + Shorter face = Likely a Cat
2 * [body pic X] + 0.5 * [face pic Y] = value 2X+0.5Y on cat axes ]</p>

<p>[X and Y are on the row of the matrix corresponding to ‘likely a cat’. They are the x values of body1 and face1’s new coords in Model 2 ]</p>

<p>What are X and Y? These are how much each feature is weighted by to calculate the score of “likely to be cat”. The higher the weight, the more that feature is taken into account for during calculation. We will reveal how these weights are related to the matrix once we get into the algebra of matrix multiplication in section [].</p>

<p>Each measurement acts as a basis vector used to define each data sample. Because this second set of measurements uses different basis vectors than face &amp; body size, it forms a different coordinate space. Since each coordinate space provides a different way to <strong>represent</strong> the data, let’s call each coordinate space a <strong>Model</strong>.</p>

<p>The face &amp; body size coordinate space will be called Model 1, and the ‘likely to be’ cat or rat model coordinate space be called Model 2. Since these two measurement methods are measuring the same data samples, the data samples in Model 1 are present in Model 2, but are now measured by different vectors.</p>

<p>In fact, since we are using Model 1 to calculate the values for Model 2, we will see in Section [] that we are applying the dot product on face &amp; body to calculate ‘likely to be cat’. Recall that the steps of matrix multiplication consists of dot products; thus, the calculation of Model 2 is none other than matrix multiplication, which was shown in this video[link] to be a rotation [footnote: see 3Bl1Br video timestamps]. Section [] will make it even more clear why matrix multiplication is called a “Change of Basis”.</p>

<!---ANIMATION: rotation w/o coordinate space [Model 1 fades on 'likely a cat' and 'likely a rat'. then it shifts.]--->

<p>For our examples, say that a vector <strong>labels</strong> a data sample if it points to it. We know in Model 1 that \(\color{#FA8072}{\begin{bmatrix} 1 \\ 0 \end{bmatrix}}\) points to <img src="/cob/face1.PNG" width="50" height="40" />. But does this vector point to the same data sample in Model 2?</p>

<p>Let’s look at the two Models. We’ll use the Orange Dot to denote <span style="color:orange">“likely to be a cat with unit 1”</span>[^1], and use the Green Dot to denote <span style="color:green">“likely to be a rat with unit 1”</span>.</p>

<!--- Animation then before/after stills:
(Use pic of actual cat with 'likely' over it?)
[coordinate space and labeled vectors don't change, and there's only 1. Only objs prev on basis vectors move.]
[as it's changing, the old basis labels shift too. the word 'body size' shifts into a non-axes vector, but the word 'likely a cat' shifts onto the basis vector. all 4 axes concepts are present.]
[ Another way to fade is to first show images, then fade away into colored dots, then move dots, and fade images back in.]
https://docs.manim.community/en/stable/reference/manim.animation.fading.FadeOut.html
Or fade out still image using video editor--->

<p><img src="/cob/fig8.PNG" alt="Figure 8" /></p>

<p>The two \(\color{#FA8072}{\begin{bmatrix} 1 \\ 0 \end{bmatrix}}\) in each Model do not label the same data point! Likewise, \(\color{#ADD8E6}{\begin{bmatrix} 0 \\ 1 \end{bmatrix}}\) points to <img src="/cob/body1.PNG" width="50" height="40" /> in Model 1, but does not point to it in Model 2. Let’s say that the <strong>meaning</strong> of a vector is the data sample it points to. \(\color{#FA8072}{\begin{bmatrix} 1 \\ 0 \end{bmatrix}}\) no longer has the same meaning in Model 2 as it did in Model 1. This is because the meaning of a vector depends on its basis vectors. In Model 1, \(\color{#FA8072}{\begin{bmatrix} 1 \\ 0 \end{bmatrix}}\) pointed to <img src="/cob/face1.PNG" width="50" height="40" /> because it’s supposed to mean “has a face with unit 1”. In Model 2, \(\color{#FA8072}{\begin{bmatrix} 1 \\ 0 \end{bmatrix}}\) points to [Orange Dot] because it’s supposed to mean “likely to be a cat with unit 1”.[^1]</p>

<p>[^1] What does it mean to have 1 unit of “likely a cat?” For our example, we generalize to allow any ‘probabilistic’ measurement to be used, as long as having X+1 units means ‘it is more likely to be” than having X units.</p>

<!---* note that [body size 1] is present in Model 2, even though it's missing [face length]. In other words, it's [body size 1] + 0 * [face length 1]. This means that any data point which only contains a body of size 1 is seen as [meaning in terms of basis jk]--->

<!---
Fig 9a,9b
[Model 1, and Model 1 on top of Model 2. Sys 2 'jk'. WITH vectors.]

in the vector on top of model pic, color them the same, but point out that they're not the same vector; they just label the same concept. [1 0] != [1.5 -1]. put numbered vector text next to each same color vector. OR make one darker than the other. Do not put basis vectors???

Disclaimer: now show with rotated vectors. Animation doesn't rotate vectors. This can get confusing because before, the vectors stayed in place. So we have to note that these are different vectors, just colored the same way because they point onto the same data point.
To prev clutter here, ONLY use 2 vectors: c and d. Or perhaps don't show them as vectors, just as dots. This should be clean and summarized.
--->

<p>If it’s still not clear why meaning depends on the choice of basis vectors, let’s look at a better example that will help drive home this idea. We know in Model 1 that \(\begin{bmatrix} 0.5 \\ 2 \end{bmatrix}\) labels <img src="/cob/cat.PNG" width="50" height="40" />. But just like before, \(\color{#CBC3E3}{\begin{bmatrix} 0.5 \\ 2 \end{bmatrix}}\) in Model 2 does not.</p>

<p><img src="/cob/fig10.PNG" alt="Figure 10" />
<!---
<img src="/cob/fig10a.png" width="300" height="200">
<img src="/cob/fig10b.png" width="300" height="200">
---></p>

<!---
[[2 0.5] catpic in Model 1 and [2 0.5] in Model 1 on 2. I vector is fixed. unlike prev anim, fade j,k only after change basis so not too cluttered]
[labels cat pic on left, and nothing on right] [color code or include pic of vector when referring to [2 0.5] in text paragraph]
--->

<p>Recall that in Model 1, which uses the face &amp; body sizes as basis vectors, \(\begin{bmatrix} 0.5 \\ 2 \end{bmatrix}\) meant “an entity with a short face (0.5) and a long body (2).” But in Model 2, this vector means “it’s not as likely to be a cat (0.5), but more likely to be a rat (2)”.</p>

<!---
In fact, $$\begin{bmatrix} 0.5 \\ 2 \end{bmatrix}$$ should now point to [rat pic], instead of <img src="/cob/cat.PNG" width="50" height="40">.

Apply inv of matrix onto [0.5, 2] to get rat pic in Model 1, which you use to construct body and face sizes

1 1.5
-1.5 1

0.30769230769230769231  -0.46153846153846153845
0.46153846153846153846  0.3076923076923076923

You get:
1   -0.76923076923076927
2   0.84615384615384618

DO THIS LATER- this is b/c this is a bad matrix to use for classif rat/cat based on body/face (used b/c rotation more intuitive. The good matrix to use is less intuitive.)

Fig ??
[now fill in what [2 0.5] is in Sys 2]
--->

<p>This shows the difference between the data samples in the real world, and the model that represents those data samples using labels. \(\begin{bmatrix} 0.5 \\ 2 \end{bmatrix}\) is not <img src="/cob/cat.PNG" width="50" height="40" /> itself; it is merely a label of it, and whichever label is used depends on the basis vectors used to define the parts of each label.</p>

<p>\(\begin{bmatrix} 0.5 \\ 2 \end{bmatrix} \neq\) <img src="/cob/cat.PNG" width="50" height="40" /></p>

<!--- Fig 11 [animated reality of concepts vs fixed coord space model]--->

<p>Notice that Model 2 demonstrates an idealized, simplified example of what a neural network does- it is making a guess about the data point given to it as input. In fact, one can think of it as a single layer ‘neural network’ such that for its neuron function:</p>

<p>o = ReLU(WX + b)</p>

<p>used to calculate the values it guesses for the 2 classes {cat, rat}, it sets ReLU = identity and b = 0, thus using the equation:</p>

<p>o = WX</p>

<p>Fig 12
[picture of X as input vector, W as arrow, O=WX as Model 2 vector on [cat pic]]</p>

<!---
Fig 13
[fading gif of changing abstractions back to actual pics; place images on coord sys]--->

<p>? * FOOTNOTE: While [cat pic] is merely how the dataset sees [actual cat pic], we are referring to [cat pic] as an “entity in the real world”. This is because the only information the neural network knows about [actual cat pic] comes from [cat pic]. For the purposes of this example, we can replace [cat pic] with [actual cat pic] to explain the same concept. In this example, [cat pic] contains information from the original dataset that does not change after the transformation- namely, body size and face length. However, in other cases, it is possible for a transformation to reduce the information previously contained, although this information may not be important.</p>

<hr />

<p>Let’s look at another example involving [poison pic and gift pic] to further illustrate the difference between the real world and our coordinate space model. Instead of using numbers, let’s use letters to label our entities.</p>

<p>[first show coordinate space labeling gift as poison]</p>

<p>To an English speaker, this may look wrong. But in German, [poison pic] is in fact called ‘gift’. If a German speaker tells the English speaker that they’re giving the English speaker a gift, the English speaker may be delighted. But they shouldn’t be, because what they’re actually receiving is [poison pic], which would kill them.</p>

<p>Instead, the English speaker needs to know what [poison pic] is actually referring to. So they need to translate from German to English as follows:</p>

<p>[animation transforming poison and gift pics to English coordinate space. The vector does not move. Label first Sys as German, second as English.]
[Don’t give names to basis vectors, ONLY show I -&gt; gift, which is wrong.]</p>

<p>label (‘gift’) in German != label (‘gift’) in English
label (‘gift’) in German ~ label (‘poison’) in English</p>

<p>Relating this back to using numbers as labels (write #s below):
label [2 0.5] (‘gift’) in German != label [2 0.5] (‘gift’) in English
label [2 0.5] (‘gift’) in German ~ label [? ?] (‘poison’) in English</p>

<p>Now if the English speaker tells the German speaker that they’re giving them a ‘gift’, the German speaker must translate this to a German translation* that makes them understand that it’s [gift pic].</p>
<ul>
  <li>not necessarily a word, but possibly a German sentence, or even a paragraph or textbook</li>
</ul>

<p>[show coordinate space w/ Geschenk]</p>

<p>[2 0.5] (Gift, German) != [2 0.5] (Gift, English)
[2 0.5] (Gift, German) ~= [-4 1] (Disgust, English)
[1/3 5/3] (Geschenk, German) ~= [2 0.5] (Gift, English)</p>

<p>Note that there is a difference between “what gift translates to” and “what gift means”. “What gift translates to in German” means what the label on [gift pic] is in English. “What gift means in German” is about what the LABEL ‘gift’ itself points to in German. The entity [gift pic] and the label ‘gift’ are not the same. They are only the same when using English, which is defined by the “English basis vectors”. More about what this means will be discussed in section X, which views basis vectors in a similar way to the Rosetta Stone.</p>

<p>“what gift translates to” : [gift pic]
“what gift means”: the label ‘gift’ (each label should be highlighted w/ diff font)</p>

<p>But what does the label ‘disgust’ mean in German? As we see in the German coordinate space, it does not point to any entity. In fact, the label ‘disgust’ does not mean anything in German. Not all labels have to point to an entity; so in some coordinate spaces, they just mean nonsense. This is an instance of ‘not confusing the map for the territory’- the map of Switzerland is not 1-1 with Switzerland itself. The model may not capture everything about reality.</p>

<p>[show a place in Switzerland not on the map]</p>

<ul>
  <li>this is an example of a False Friend. link</li>
</ul>

<p>«&lt;
Now that we understand the difference between entities and labels, let’s look back at our example with cats and rats. Remember that in Model 2, the vector I no longer labels the entity [cat pic]; in Model 2, it’s the vector O that labels [cat pic].</p>

<p>[put vector O on coord sys]</p>

<p>How do we calculate what the new label for [cat] is? As we’ll soon see in section X, the answer is found using Matrix-Vector Multiplication.</p>

<p>«&lt;
MOVE TO 1.1+:
Note that the relationships between the entities doesn’t change. Since the basis vectors from Model 1 still exist in Model 2, you can still use the instructions from Model 1, but you have to translate them using the change of basis matrix.
… As we’ll see in section X, this is why the dot product instructions work.</p>

<p>section Y will delve deeper into the relationships between concepts, and how they can be preserved or destroyed via matrix multiplication. It retrieves insights that are crucial for…</p>

<p>While [cat pic entity] is a representation of a entity that exists in the real world, 
Relationships are preserved. Analogy. Structure preserving map
[Maps between 2 domains: the real world, and the coordinate space]</p>

:ET